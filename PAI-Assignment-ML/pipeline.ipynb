{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88aba3f-ab4d-4224-984e-b88aceaa9151",
   "metadata": {},
   "source": [
    "## **Waleed Umar (SP25-RAI-021)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8e875-8fe5-49c8-aa57-b2fe130da95c",
   "metadata": {},
   "source": [
    "# **Paper Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734a981-20ae-496c-be64-cabbda1d27b2",
   "metadata": {},
   "source": [
    "**The paper selected was \"Long-Term Coronary Artery Disease Risk Prediction with Machine Learning Models\", published on 20 January 2023, in the HEC W Category ranked journal, Sensors.**\n",
    "\n",
    "The paper is linked [here](https://www.mdpi.com/1424-8220/23/3/1193)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d543dc0-e0d0-4ea0-b8b6-d349b7db5035",
   "metadata": {},
   "source": [
    "# **Restructuring the Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "064fc5e6-41f1-4573-9ab3-d72a19b47650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Results:\n",
      "                             Accuracy  Precision    Recall       AUC\n",
      "3-NN                         0.812500   0.213115  0.104839  0.559426\n",
      "Bagging                      0.853774   0.500000  0.072581  0.691956\n",
      "Decision Tree (J48)          0.737028   0.163265  0.193548  0.511829\n",
      "Logistic Regression (LR)     0.859670   0.666667  0.080645  0.704732\n",
      "MLP (Multilayer Perceptron)  0.853774   0.000000  0.000000  0.500000\n",
      "Naive Bayes (NB)             0.824292   0.313433  0.169355  0.711326\n",
      "Random Forest (RF)           0.856132   0.555556  0.080645  0.685534\n",
      "Stacking                     0.853774   0.500000  0.096774  0.684816\n",
      "Voting                       0.824292   0.295082  0.145161  0.691120\n",
      "\n",
      "SMOTE Data Results:\n",
      "                             Accuracy  Precision    Recall       AUC\n",
      "3-NN                         0.814325   0.723586  0.988338  0.907344\n",
      "Bagging                      0.901947   0.913505  0.877551  0.962532\n",
      "Decision Tree (J48)          0.840751   0.815172  0.861516  0.841662\n",
      "Logistic Regression (LR)     0.664812   0.641274  0.674927  0.730569\n",
      "MLP (Multilayer Perceptron)  0.522949   0.000000  0.000000  0.500000\n",
      "Naive Bayes (NB)             0.624478   0.712209  0.357143  0.732910\n",
      "Random Forest (RF)           0.911683   0.933333  0.877551  0.967036\n",
      "Stacking                     0.913074   0.933539  0.880466  0.966843\n",
      "Voting                       0.808067   0.866071  0.706997  0.909892\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "df = pd.read_csv('coronary_prediction.csv')\n",
    "\n",
    "def impute_static_values(df):\n",
    "    df['education'] = df['education'].fillna(1)\n",
    "    df['totChol'] = df['totChol'].fillna(236)\n",
    "    df['BMI'] = df['BMI'].fillna(25.8)\n",
    "    df['heartRate'] = df['heartRate'].fillna(75)\n",
    "    return df[['education', 'totChol', 'BMI', 'heartRate']]\n",
    "\n",
    "def impute_cigs_per_day(df):\n",
    "    df['cigsPerDay'] = df['cigsPerDay'].fillna(9999)\n",
    "    df.loc[df['cigsPerDay'] == 9999, 'cigsPerDay'] = df['currentSmoker'].apply(lambda x: 18 if x == 1 else 0)\n",
    "    return df[['cigsPerDay']]\n",
    "\n",
    "def impute_bpmeds(df):\n",
    "    df['BPMeds'] = df['BPMeds'].fillna(9999)\n",
    "    df.loc[df['BPMeds'] == 9999, 'BPMeds'] = df['prevalentHyp'].apply(lambda x: 1 if x == 1 else 0)\n",
    "    return df[['BPMeds']]\n",
    "\n",
    "def impute_glucose(df):\n",
    "    df['glucose'] = df['glucose'].fillna(9999)\n",
    "    df.loc[df['glucose'] == 9999, 'glucose'] = df['diabetes'].apply(lambda x: 170 if x == 1 else 79)\n",
    "    return df[['glucose']]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('static', FunctionTransformer(impute_static_values), ['education', 'totChol', 'BMI', 'heartRate']),\n",
    "        ('cigs_per_day', FunctionTransformer(impute_cigs_per_day), ['cigsPerDay', 'currentSmoker']),\n",
    "        ('bpmeds', FunctionTransformer(impute_bpmeds), ['BPMeds', 'prevalentHyp']),\n",
    "        ('glucose', FunctionTransformer(impute_glucose), ['glucose', 'diabetes'])\n",
    "    ],\n",
    "    remainder='passthrough'  \n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"3-NN\": KNeighborsClassifier(n_neighbors=3, algorithm='auto', metric='euclidean'),\n",
    "    \"Bagging\": BaggingClassifier(RandomForestClassifier(n_estimators=500, random_state=42), n_estimators=10, random_state=42),\n",
    "    \"Decision Tree (J48)\": DecisionTreeClassifier(random_state=42, criterion='gini', splitter='best', max_depth=None),\n",
    "    \"Logistic Regression (LR)\": LogisticRegression(C=1e8, solver='lbfgs', max_iter=3000, random_state=42),\n",
    "    \"MLP (Multilayer Perceptron)\": MLPClassifier(learning_rate_init=0.1, momentum=0.2, max_iter=200, random_state=42),\n",
    "    \"Naive Bayes (NB)\": GaussianNB(),\n",
    "    \"Random Forest (RF)\": RandomForestClassifier(n_estimators=500, oob_score=True, random_state=42, max_samples=1.0),\n",
    "     \"Stacking\": StackingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('nb', GaussianNB())\n",
    "    ], final_estimator=LogisticRegression(C=1e8, solver='lbfgs', random_state=42)),\n",
    "    \"Voting\": VotingClassifier(estimators=[\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('nb', GaussianNB())\n",
    "    ], voting='soft')\n",
    "   \n",
    "}\n",
    "\n",
    "def evaluate_metrics(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "results = {\"Original Data\": {}, \"SMOTE Data\": {}}\n",
    "\n",
    "X = df.drop('TenYearCHD', axis=1)  \n",
    "y = df['TenYearCHD']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# FOR SMOTE ON WHOLE DATA\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "smote = SMOTE(k_neighbors=5, random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_processed, y)\n",
    "\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "    \n",
    "for name, model in models.items():\n",
    "    # Original data pipeline\n",
    "    original_pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    results[\"Original Data\"][name] = evaluate_metrics(original_pipeline, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "   # for smote stuff\n",
    "    results[\"SMOTE Data\"][name] = evaluate_metrics(model, X_train_smote, X_test_smote, y_train_smote, y_test_smote)\n",
    "\n",
    "# Converting the results to DataFrames\n",
    "original_results_df = pd.DataFrame(results[\"Original Data\"]).T\n",
    "smote_results_df = pd.DataFrame(results[\"SMOTE Data\"]).T\n",
    "\n",
    "\n",
    "print(\"Original Data Results:\")\n",
    "print(original_results_df)\n",
    "print(\"\\nSMOTE Data Results:\")\n",
    "print(smote_results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
